debug: false
use_random_seed: false
seed: 461

dataset:
  data_dir: data/llm_data/mcq_mix_v6

lora:
    rank: 64
    alpha: 16
    dropout: 0.05
    bias: 'none'
    target_modules: [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",
    ]
    task_type: "CAUSAL_LM"

quant:
  use_4bit: true
  quant_type: "nf4"
  use_double_quant: true
  use_bf16: true

model:
    model_name: mistralai/Mixtral-8x7B-Instruct-v0.1
    max_length: 1024
    model_output_dir: results/llm/Mixtral-8x7B-Instruct-v0.1

train:
  num_epochs: 3
  batch_size: 4  # per_device
  learning_rate: 2e-5
  eval_steps: 1000
  lr_scheduler_type: cosine
  warmup_ratio: 0.1
  gradient_accumulation_steps: 2
  train_data_file: data/llm_data/mcq_mix_v6/train_mix_6.jsonl
  val_data_file: data/llm_data/mcq_mix_v6/valid_mix_6.jsonl

eval:
  eval_data_file: data/llm_data/mcq_mix_v6/valid_mix_6.jsonl